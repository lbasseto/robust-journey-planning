{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import calendar\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as functions\n",
    "import math\n",
    "import getpass\n",
    "import pyspark\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "import networkx as nx\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName('journey_planner-{0}'.format(getpass.getuser())) \\\n",
    "    .config('spark.jars.packages', 'graphframes:graphframes:0.6.0-spark2.3-s_2.11') \\\n",
    "    .config('spark.executor.memory', '8g') \\\n",
    "    .config('spark.executor.instances', '5') \\\n",
    "    .config('spark.port.maxRetries', '100') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate vertices and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = spark.read.csv('/datasets/sbb/2018/*/*istdaten.csv.bz2', sep=';', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations = pd.read_csv('data/filtered_stations.csv')\n",
    "valid_stations = set(stations['Remark'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vertices_df = stations[['Remark', 'Longitude', 'Latitude']]\n",
    "vertices_df.columns = ['id', 'lon', 'lat']\n",
    "vertices = spark.createDataFrame(vertices_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations = stations[['Longitude', 'Latitude', 'Remark']];\n",
    "stations['key'] = 0\n",
    "\n",
    "earth_radius = 6371e3\n",
    "\n",
    "def haversine(row):\n",
    "    phi1         = 2 * math.pi * float(row['Latitude_x']) / 360\n",
    "    phi2         = 2 * math.pi * float(row['Latitude_y']) / 360\n",
    "    delta_phi    = 2 * math.pi * (float(row['Latitude_y']) - float(row['Latitude_x'])) / 360\n",
    "    delta_lambda = 2 * math.pi * (float(row['Longitude_y']) - float(row['Longitude_x'])) / 360\n",
    "    \n",
    "    a = (math.sin(delta_phi/2) ** 2) + \\\n",
    "        math.cos(phi1) * math.cos(phi2) * (math.sin(delta_lambda/2) ** 2)\n",
    "    \n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    \n",
    "    d = earth_radius * c\n",
    "    \n",
    "    return d / 1000\n",
    "\n",
    "prod = pd.merge(stations, stations, on='key')\n",
    "prod['dist'] = prod.apply(lambda row: haversine(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We don't consider walking to stops that are more than 1 kilometers away\n",
    "max_walking_distance = 1\n",
    "walk_df = prod[prod['dist'] <= max_walking_distance]\n",
    "walk_df = walk_df[walk_df['Remark_x'] != walk_df['Remark_y']]\n",
    "\n",
    "walk_df = walk_df[['Remark_x', 'Remark_y', 'dist']]\n",
    "walk_df['type'] = 'walk'\n",
    "walk_df['line'] = 'walk'\n",
    "walk_df['departure_day']  = 'null'\n",
    "walk_df['departure_time'] = 'null'\n",
    "walk_df['arrival_time']   = 'null'\n",
    "# We assume an average walking speed of 5 kilometers per hour\n",
    "walk_df['lateAvg'] = walk_df.apply(lambda row: 3600 * float(row['dist']) / 5, axis=1)\n",
    "walk_df['lateStd'] = 0.0\n",
    "walk_df.drop('dist', axis=1, inplace=True)\n",
    "walk_df.columns = ['src', 'dst', 'type', 'line', 'departure_day', 'departure_time', 'arrival_time', 'lateAvg', 'lateStd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "walk_edges = spark.createDataFrame(walk_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transport edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dateFormat = 'dd.MM.yyyy HH:mm'\n",
    "timeLate = (functions.unix_timestamp('AN_PROGNOSE', format=dateFormat)\n",
    "            - functions.unix_timestamp('ANKUNFTSZEIT', format=dateFormat))\n",
    "\n",
    "@functions.udf\n",
    "def clamp(late):\n",
    "    return 0 if late < 0 else late\n",
    "\n",
    "valid_stops = df.filter((df.DURCHFAHRT_TF=='false') & \n",
    "                        (df.FAELLT_AUS_TF=='false') & \n",
    "                        (df.ZUSATZFAHRT_TF=='false') &\n",
    "                        (df.AN_PROGNOSE_STATUS=='GESCHAETZT') &\n",
    "                        (df.HALTESTELLEN_NAME.isin(valid_stations))) \\\n",
    "                .select('BETRIEBSTAG',\n",
    "                        'FAHRT_BEZEICHNER', \n",
    "                        'PRODUKT_ID', \n",
    "                        'LINIEN_TEXT', \n",
    "                        'HALTESTELLEN_NAME', \n",
    "                        'AN_PROGNOSE',\n",
    "                        'ANKUNFTSZEIT', \n",
    "                        'ABFAHRTSZEIT') \\\n",
    "                .withColumn('AN_PROGNOSE',  functions.to_timestamp(df.AN_PROGNOSE, dateFormat))  \\\n",
    "                .withColumn('ANKUNFTSZEIT', functions.to_timestamp(df.ANKUNFTSZEIT, dateFormat)) \\\n",
    "                .withColumn('ABFAHRTSZEIT', functions.to_timestamp(df.ABFAHRTSZEIT, dateFormat)) \\\n",
    "                .withColumn('late', clamp(timeLate)) \\\n",
    "                .drop('AN_PROGNOSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "departures = valid_stops.filter(valid_stops.ABFAHRTSZEIT.isNotNull())\\\n",
    "                        .drop('ANKUNFTSZEIT', 'late')\n",
    "arrivals   = valid_stops.filter(valid_stops.ANKUNFTSZEIT.isNotNull())\\\n",
    "                        .drop('ABFAHRTSZEIT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arrivals.createOrReplaceTempView('arrivals')\n",
    "departures.createOrReplaceTempView('departures')\n",
    "\n",
    "joinQuery = 'SELECT d.HALTESTELLEN_NAME AS src, a.HALTESTELLEN_NAME AS dst,              \\\n",
    "                    d.PRODUKT_ID AS type, d.LINIEN_TEXT AS line,                         \\\n",
    "                    date_format(d.ABFAHRTSZEIT, \\'EEEE\\') AS departure_day,              \\\n",
    "                    SUBSTRING(d.ABFAHRTSZEIT, 12, 8) AS departure_time,                  \\\n",
    "                    SUBSTRING(a.ANKUNFTSZEIT, 12, 8) AS arrival_time,                    \\\n",
    "                    a.late                                                               \\\n",
    "             FROM arrivals AS a INNER JOIN departures AS d                               \\\n",
    "             ON a.BETRIEBSTAG == d.BETRIEBSTAG                                           \\\n",
    "             AND a.FAHRT_BEZEICHNER == d.FAHRT_BEZEICHNER                                \\\n",
    "             WHERE a.HALTESTELLEN_NAME != d.HALTESTELLEN_NAME                            \\\n",
    "             AND d.ABFAHRTSZEIT < a.ANKUNFTSZEIT'\n",
    "\n",
    "edges = spark.sql(joinQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edges.createOrReplaceTempView('edges')\n",
    "\n",
    "query = 'SELECT src, dst, type, line, departure_day, departure_time, arrival_time,              \\\n",
    "         AVG(late) AS lateAvg, STD(late) AS lateStd                                             \\\n",
    "         FROM edges GROUP BY src, dst, type, line, departure_day, departure_time, arrival_time'\n",
    "\n",
    "aggregated = spark.sql(query)\n",
    "aggregated_edges = aggregated.na.fill(0.0)\n",
    "\n",
    "all_edges = aggregated_edges.union(walk_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1457.parquet.\n: org.apache.hadoop.security.AccessControlException: Permission denied: user=kgerard, access=WRITE, inode=\"/homes/schmutz\":schmutz:hadoop:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:258)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1857)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:110)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3002)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1095)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:692)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n\tat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1603)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:953)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:950)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:960)\n\tat org.apache.spark.internal.io.FileCommitProtocol.deleteWithJob(FileCommitProtocol.scala:123)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:210)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:117)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:549)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=kgerard, access=WRITE, inode=\"/homes/schmutz\":schmutz:hadoop:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:258)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1857)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:110)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3002)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1095)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:692)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1443)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1353)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy11.delete(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:634)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy12.delete(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1601)\n\t... 37 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-69aaa95eb284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_edges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/homes/schmutz/edges'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1457.parquet.\n: org.apache.hadoop.security.AccessControlException: Permission denied: user=kgerard, access=WRITE, inode=\"/homes/schmutz\":schmutz:hadoop:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:258)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1857)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:110)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3002)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1095)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:692)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n\tat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1603)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:953)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:950)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:960)\n\tat org.apache.spark.internal.io.FileCommitProtocol.deleteWithJob(FileCommitProtocol.scala:123)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:210)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:117)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:549)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=kgerard, access=WRITE, inode=\"/homes/schmutz\":schmutz:hadoop:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:258)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1857)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:110)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3002)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1095)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:692)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1443)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1353)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy11.delete(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:634)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy12.delete(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1601)\n\t... 37 more\n"
     ]
    }
   ],
   "source": [
    "all_edges.write.parquet('/homes/schmutz/edges', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vertices.write.parquet('/homes/schmutz/vertices', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vertices = spark.read.parquet('/homes/schmutz/vertices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+-----------------+\n",
      "|                  id|              lon|              lat|\n",
      "+--------------------+-----------------+-----------------+\n",
      "|   Zumikon, Gössikon|         8.614773|        47.332474|\n",
      "|   Zumikon, Waltikon|         8.618188|        47.336109|\n",
      "|Zumikon, Dorfzentrum|         8.622922|        47.332976|\n",
      "|Zürich, Meierhofp...|         8.499375|        47.402009|\n",
      "|  Zürich, Heizenholz|8.483903999999999|47.41229600000001|\n",
      "+--------------------+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vertices.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edges = spark.read.parquet('/homes/schmutz/edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+----+-------------+--------------+------------+-----------------+-------+\n",
      "|                 src|                 dst|type|line|departure_day|departure_time|arrival_time|          lateAvg|lateStd|\n",
      "+--------------------+--------------------+----+----+-------------+--------------+------------+-----------------+-------+\n",
      "|Zürich, Carl-Spit...|Zürich, Stodolast...|walk|walk|         null|          null|        null|243.1882193019385|    0.0|\n",
      "|Zürich, Carl-Spit...|      Zürich, Rehalp|walk|walk|         null|          null|        null|  616.09711715276|    0.0|\n",
      "|Zürich, Carl-Spit...|     Zürich, Segeten|walk|walk|         null|          null|        null|401.5413867498982|    0.0|\n",
      "|Zürich, Carl-Spit...|Zürich, Waserstrasse|walk|walk|         null|          null|        null|227.3430054944393|    0.0|\n",
      "|Zürich, Carl-Spit...| Zürich, Wiesliacher|walk|walk|         null|          null|        null|607.4434392971077|    0.0|\n",
      "+--------------------+--------------------+----+----+-------------+--------------+------------+-----------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Journey Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = nx.MultiDiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.add_edge('e',3, wt={'yo':4,'pp':4})\n",
    "G.add_edge(45,3, wt={'yo':34,'pp':4})\n",
    "G.add_edge(45,3, wt={'yo':34,'pp':4})\n",
    "G.add_edge(4225,3, wt={'yo':34,'pp':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMultiEdgeDataView([('e', 3), (45, 3), (45, 3), (4225, 3)])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = G.in_edges(3)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n",
      "{0: {'wt': {'yo': 4, 'pp': 4}}}\n",
      "45\n",
      "{0: {'wt': {'yo': 34, 'pp': 4}}, 1: {'wt': {'yo': 34, 'pp': 4}}}\n",
      "{'wt': {'yo': 34, 'pp': 4}}\n",
      "45\n",
      "{0: {'wt': {'yo': 34, 'pp': 4}}, 1: {'wt': {'yo': 34, 'pp': 4}}}\n",
      "{'wt': {'yo': 34, 'pp': 4}}\n",
      "4225\n",
      "{0: {'wt': {'yo': 34, 'pp': 4}}}\n"
     ]
    }
   ],
   "source": [
    "v = G.in_edges(3, data=True)\n",
    "for e in v:\n",
    "    print(e[0])\n",
    "    print(G[e[0]][3])\n",
    "    if 1 in G[e[0]][3]:\n",
    "        print(G[e[0]][3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MINUTES_PER_DAY = 1440\n",
    "MINUTES_PER_HOUR = 60\n",
    "SECONDS_PER_MINUTE = 60\n",
    "\n",
    "def computeDiff(departure, arrival):\n",
    "    dep = (departure[2:]).split(':')\n",
    "    arr = (arrival[2:]).split(':')\n",
    "    a = (int(arrival[:1]) - int(departure[:1])) * MINUTES_PER_DAY\n",
    "    b = (int(arr[0]) - int(dep[0])) * MINUTES_PER_HOUR\n",
    "    c = (int(arr[1]) - int(dep[1]))\n",
    "    tot = a + b + c\n",
    "    hours = tot // MINUTES_PER_HOUR\n",
    "    minutes = tot % MINUTES_PER_HOUR\n",
    "    return \"{:02d}\".format(int(hours)) + ':' + \"{:02d}\".format(int(minutes)) + ':00'\n",
    "\n",
    "def computeCost(cost, late):\n",
    "    tmp = cost.split(':')\n",
    "    a = int(tmp[0][2:]) * MINUTES_PER_HOUR + int(tmp[1])\n",
    "    b = late // SECONDS_PER_MINUTE\n",
    "    prefix = tmp[0][:2] if a > b else '0-'\n",
    "    a = (a - b) % MINUTES_PER_DAY\n",
    "    minutes = a % MINUTES_PER_HOUR\n",
    "    hours = (a - minutes) // MINUTES_PER_HOUR\n",
    "    return prefix + \"{:02d}\".format(int(hours)) + ':' + \"{:02d}\".format(int(minutes)) + ':00'\n",
    "\n",
    "def getFilteredEdges(startDay, finishDay, startTime, finishTime, duration):\n",
    "    def valid(day, depTime, arrTime, walkTime):\n",
    "        if startDay==finishDay:\n",
    "            return ((day=='null') & (walkTime<=duration)) | \\\n",
    "                    ((day==startDay) & (depTime>=startTime) & (arrTime<=finishTime) & (depTime<=arrTime))\n",
    "        else:\n",
    "            return ((day=='null') & (walkTime<=duration)) | \\\n",
    "                    (((day==startDay) & (depTime>=startTime) & ((depTime<=arrTime) | (arrTime<=finishTime))) | \\\n",
    "                     ((day==finishDay) & (depTime<finishTime) & (arrTime<=finishTime)))\n",
    "\n",
    "    return graph.filterEdges(valid(graph.edges.departure_day, \n",
    "                                graph.edges.departure_time,\n",
    "                                graph.edges.arrival_time,\n",
    "                                graph.edges.lateAvg)).edges\n",
    "\n",
    "def add_vertice_to_set(max_set, vertice, vertice_costs, edges, next_vertices):\n",
    "    \n",
    "    max_set.add(vertice)\n",
    "    cost = vertice_costs[vertice]\n",
    "    \n",
    "    vertice_edges = edges.in_edges(vertice, data=True)\n",
    "    \n",
    "    for parallel_paths in vertice_edges:\n",
    "        i = 0\n",
    "        edge = parallel_paths[2]['edge']\n",
    "        if edge['type'] == 'walk':\n",
    "            new_cost = computeCost(cost, edge['lateAvg'])\n",
    "            if edge['src'] not in vertice_costs or new_cost > vertice_costs[edge.dst]:\n",
    "                next_vertices[edge['src']] = edge\n",
    "                vertice_costs[edge.src] = new_cost\n",
    "        elif edge['arrival_time'] < cost and (edge['src'] not in vertice_costs or edge['departure_time'] > vertice_costs[edge['dst']]):\n",
    "            vertice_costs[edge['src']] = edge['departure_time']\n",
    "            next_vertices[edge['src']] = edge\n",
    "        i += 1\n",
    "\n",
    "def get_max_vertice_not_in_set(max_set, vertice_costs, min_trip_departure_time):\n",
    "    max_vertice = None\n",
    "    max_cost = min_trip_departure_time\n",
    "    for vertice in vertice_costs:\n",
    "        if vertice not in max_set and vertice_costs[vertice] > max_cost:\n",
    "            max_cost = vertice_costs[vertice]\n",
    "            max_vertice = vertice\n",
    "    \n",
    "    return max_vertice\n",
    "\n",
    "def find_path(next_vertices, current_vertice, current_path):\n",
    "    if current_vertice not in next_vertices:\n",
    "        return current_path\n",
    "    next_vertice = next_vertices[current_vertice]['dst']\n",
    "    current_path.append(next_vertices[current_vertice])\n",
    "    return find_path(next_vertices, next_vertice, current_path)\n",
    "    \n",
    "\n",
    "def find_shortest_path(departure_station, arrival_station, \n",
    "                       startDateTime, endDateTime, \n",
    "                       min_probability_of_sucess):\n",
    "    \n",
    "    print(startDateTime)\n",
    "    print(endDateTime)\n",
    "    \n",
    "    startTime  = str(startDateTime.time())\n",
    "    endTime = str(endDateTime.time())\n",
    "\n",
    "    startDay  = calendar.day_name[startDateTime.weekday()]\n",
    "    endDay = calendar.day_name[endDateTime.weekday()]\n",
    "    \n",
    "    min_trip_departure_time = '0-' + startTime\n",
    "    \n",
    "    endTimePrefix = '0-' if (startDay == endDay) else '1-'\n",
    "    requested_arrival_time = endTimePrefix + endTime\n",
    "    \n",
    "    duration = (endDateTime - startDateTime).seconds\n",
    "    \n",
    "    print(startDay, startTime)\n",
    "    print(endDay, endTime)\n",
    "    \n",
    "    filtered_edges = getFilteredEdges(startDay, endDay, startTime, endTime, duration).toPandas()\n",
    "    \n",
    "    def to_dt(time):\n",
    "        if time == 'null':\n",
    "            return 'null'\n",
    "        elif time >= startTime:\n",
    "            return '0-' + time\n",
    "        else:\n",
    "            return '1-' + time\n",
    "\n",
    "\n",
    "    \n",
    "    filtered_edges['departure_time'] = filtered_edges['departure_time'].map(lambda x: to_dt(x))\n",
    "    filtered_edges['arrival_time']   = filtered_edges['arrival_time'].map(lambda x: to_dt(x))\n",
    "    \n",
    "    G = nx.MultiDiGraph()\n",
    "    \n",
    "    for i, edge in filtered_edges.iterrows():\n",
    "        G.add_edge(edge['src'], edge['dst'], edge=edge)\n",
    "        G.add_edge(edge['src'], edge['dst'], edge=edge)\n",
    "    \n",
    "\n",
    "    \n",
    "    # as day#-hh-mm-ss\n",
    "    vertice_costs = {}\n",
    "    vertice_costs[arrival_station] = requested_arrival_time\n",
    "\n",
    "    max_set = set()\n",
    "    next_vertices = {}\n",
    "    add_vertice_to_set(max_set, arrival_station, vertice_costs, G, next_vertices)\n",
    "    no_solution = False\n",
    "    print(\"data loaded\", time.time() - starttime)\n",
    "    while(departure_station not in max_set and not no_solution):\n",
    "        max_vertice = get_max_vertice_not_in_set(max_set, vertice_costs, min_trip_departure_time)\n",
    "        if max_vertice is None:\n",
    "            no_solution = True\n",
    "        else:\n",
    "            add_vertice_to_set(max_set, max_vertice, vertice_costs, G, next_vertices)\n",
    "    \n",
    "    if no_solution:\n",
    "        print(\"no solution\", vertice_costs)\n",
    "    \n",
    "    departure_time = (vertice_costs[departure_station])[2:]\n",
    "        \n",
    "    trip_duration = computeDiff(vertice_costs[departure_station], requested_arrival_time)\n",
    "    \n",
    "    return departure_time, trip_duration, find_path(next_vertices, departure_station, [departure_station])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-03 18:16:00\n",
      "2019-06-03 20:57:00\n",
      "Monday 18:16:00\n",
      "Monday 20:57:00\n",
      "data loaded 4.670464754104614\n",
      "5.929484128952026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('19:39:00', '01:18:00', ['Kilchberg', src                 Kilchberg\n",
       "  dst               Wallisellen\n",
       "  type                      Zug\n",
       "  line                       S8\n",
       "  departure_day          Monday\n",
       "  departure_time     0-19:39:00\n",
       "  arrival_time       0-20:03:00\n",
       "  lateAvg                    20\n",
       "  lateStd               29.1043\n",
       "  Name: 18025, dtype: object, src               Wallisellen\n",
       "  dst                    Urdorf\n",
       "  type                      Zug\n",
       "  line                      S14\n",
       "  departure_day          Monday\n",
       "  departure_time     0-20:07:00\n",
       "  arrival_time       0-20:29:00\n",
       "  lateAvg                     0\n",
       "  lateStd                     0\n",
       "  Name: 17762, dtype: object, src                                 Urdorf\n",
       "  dst               Urdorf, Schlierenstrasse\n",
       "  type                                  walk\n",
       "  line                                  walk\n",
       "  departure_day                         null\n",
       "  departure_time                        null\n",
       "  arrival_time                          null\n",
       "  lateAvg                            626.844\n",
       "  lateStd                                  0\n",
       "  Name: 8279, dtype: object])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fromStation = 'Kilchberg'\n",
    "toStation   = 'Urdorf, Schlierenstrasse'\n",
    "startDateTime = datetime(2019, 6, 3, 18, 16)\n",
    "endDateTime   = datetime(2019, 6, 3, 20, 57)\n",
    "starttime = time.time()\n",
    "res = find_shortest_path(fromStation, toStation, \n",
    "                   startDateTime, \n",
    "                   endDateTime, 0)\n",
    "print(time.time() - starttime)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARCHIVED CODE FOR POSSIBLE REUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSubGraph(graph, startDay, finishDay, startTime, finishTime, duration):\n",
    "    def valid(day, depTime, arrTime, walkTime):\n",
    "        if startDay==finishDay:\n",
    "            return ((day=='null') & (walkTime<=duration)) | \\\n",
    "                    ((day==startDay) & (depTime>=startTime) & (arrTime<=finishTime) & (depTime<=arrTime))\n",
    "        else:\n",
    "            return ((day=='null') & (walkTime<=duration)) | \\\n",
    "                    (((day==startDay) & (depTime>=startTime) & ((depTime<=arrTime) | (arrTime<=finishTime))) | \\\n",
    "                     ((day==finishDay) & (depTime<finishTime) & (arrTime<=finishTime)))\n",
    "\n",
    "    return graph.filterEdges(valid(graph.edges.departure_day, \n",
    "                                graph.edges.departure_time,\n",
    "                                graph.edges.arrival_time,\n",
    "                                graph.edges.lateAvg))  \\\n",
    "                .dropIsolatedVertices()\n",
    "\n",
    "def howFarNaive(graph, fromStation, startDateTime, duration):\n",
    "    \n",
    "    if duration >= 120:\n",
    "        print('You can walk anywhere in that time')\n",
    "        return\n",
    "    \n",
    "    finishDateTime = startDateTime + timedelta(minutes=duration)\n",
    "\n",
    "    startTime  = str(startDateTime.time())\n",
    "    finishTime = str(finishDateTime.time())\n",
    "\n",
    "    startDay  = calendar.day_name[startDateTime.weekday()]\n",
    "    finishDay = calendar.day_name[finishDateTime.weekday()]\n",
    "\n",
    "    print(startDay, startTime)\n",
    "    print(finishDay, finishTime)\n",
    "    \n",
    "    @functions.udf\n",
    "    def addTime(arr_time, dep_time, late):\n",
    "        if arr_time=='null':\n",
    "            tmp = dep_time.split(':')\n",
    "            return str((datetime.combine(date.today(), dt.time(int(tmp[0]), int(tmp[1]), int(tmp[2]))) + \n",
    "                        timedelta(seconds=int(late))).time())\n",
    "        else:\n",
    "            return arr_time\n",
    "    \n",
    "    @functions.udf\n",
    "    def checkDay(day, dep_time, arr_time):\n",
    "        return finishDay if arr_time<dep_time else day\n",
    "    \n",
    "    @functions.udf\n",
    "    def checkWalk(ttype):\n",
    "        return 1 if ttype=='walk' else 0\n",
    "    \n",
    "    @functions.udf\n",
    "    def checkIfValid(arr_time, day):\n",
    "        tmp = arr_time.split(':')\n",
    "        arr_date = startDateTime.date() if day==calendar.day_name[startDateTime.weekday()] else finishDateTime.date()\n",
    "        arrival = datetime.combine(arr_date, dt.time(int(tmp[0]), int(tmp[1]), int(tmp[2])))\n",
    "        return arrival < finishDateTime\n",
    "    \n",
    "    reachable = vertices.filter(vertices.id==fromStation)             \\\n",
    "                        .withColumn('time', functions.lit(startTime)) \\\n",
    "                        .withColumn('day', functions.lit(startDay))   \\\n",
    "                        .withColumn('just_walked', functions.lit(0))\n",
    "    \n",
    "    g = getSubGraph(graph, startDay, finishDay, startTime, finishTime, 60*duration)\n",
    "    g.persist();\n",
    "    g.edges.createOrReplaceTempView('edges')\n",
    "    g.vertices.createOrReplaceTempView('vertices')\n",
    "    \n",
    "    curr = reachable\n",
    "    \n",
    "    #while len(curr.head(1)) > 0:\n",
    "    for i in range(1):\n",
    "        curr.createOrReplaceTempView('curr')\n",
    "\n",
    "        query = 'SELECT v.*, c.time AS past_time, c.just_walked, c.day, e.type,          \\\n",
    "                        e.departure_time, e.arrival_time, e.lateAvg                      \\\n",
    "                 FROM curr AS c INNER JOIN edges AS e INNER JOIN vertices AS v           \\\n",
    "                 ON c.id==e.src                                                          \\\n",
    "                 AND e.dst==v.id                                                         \\\n",
    "                 WHERE (e.type!=\\'walk\\' OR c.just_walked==0)                            \\\n",
    "                 AND (e.type==\\'walk\\'                                                   \\\n",
    "                 OR (e.departure_time>=c.time AND c.day==e.departure_day)                \\\n",
    "                 OR (e.departure_time<c.time AND c.day!=e.departure_day))'\n",
    "\n",
    "        curr = spark.sql(query).withColumn('time', addTime('arrival_time', 'past_time', 'lateAvg')) \\\n",
    "                               .withColumn('day', checkDay('day', 'past_time', 'time'))             \\\n",
    "                               .filter(checkIfValid('time', 'day')=='true')                         \\\n",
    "                               .withColumn('just_walked', checkWalk('type'))                        \\\n",
    "                               .select('id', 'lon', 'lat', 'time', 'day', 'just_walked')\n",
    "        curr.persist()\n",
    "        reachable = reachable.union(curr)\n",
    "    \n",
    "    @functions.udf\n",
    "    def computeRadius(arr_time, day):\n",
    "        tmp = arr_time.split(':')\n",
    "        arr_date = startDateTime.date() if day==calendar.day_name[startDateTime.weekday()] else finishDateTime.date()\n",
    "        arrival = datetime.combine(arr_date, dt.time(int(tmp[0]), int(tmp[1]), int(tmp[2])))\n",
    "        return (finishDateTime - arrival).seconds * 5 // 3.6\n",
    "    \n",
    "    reachable = reachable.withColumn('radius', computeRadius('time', 'day')) \\\n",
    "                         .select('id', 'lon', 'lat', 'radius', 'time', 'just_walked')                     \\\n",
    "                         .toPandas()\n",
    "    \n",
    "    g.unpersist();\n",
    "    \n",
    "    return reachable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = GraphFrame(vertices, edges)\n",
    "fromStation = 'Dietlikon'\n",
    "startDateTime  = datetime(2019, 5, 31, 23, 45)\n",
    "duration = 60\n",
    "\n",
    "start = time.time()\n",
    "reachable = howFarNaive(graph, fromStation, startDateTime, duration)\n",
    "print(time.time() - start)\n",
    "reachable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
