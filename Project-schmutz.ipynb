{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import calendar\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stat\n",
    "import pyspark.sql.functions as functions\n",
    "import math\n",
    "import getpass\n",
    "import pyspark\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "import networkx as nx\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName('journey_planner-{0}'.format(getpass.getuser())) \\\n",
    "    .config('spark.jars.packages', 'graphframes:graphframes:0.6.0-spark2.3-s_2.11') \\\n",
    "    .config('spark.executor.memory', '8g') \\\n",
    "    .config('spark.executor.instances', '5') \\\n",
    "    .config('spark.port.maxRetries', '100') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate vertices and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = spark.read.csv('/datasets/sbb/2018/*/*istdaten.csv.bz2', sep=';', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations = pd.read_csv('data/filtered_stations.csv')\n",
    "valid_stations = set(stations['Remark'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vertices_df = stations[['Remark', 'Longitude', 'Latitude']]\n",
    "vertices_df.columns = ['id', 'lon', 'lat']\n",
    "vertices = spark.createDataFrame(vertices_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations = stations[['Longitude', 'Latitude', 'Remark']];\n",
    "stations['key'] = 0\n",
    "\n",
    "earth_radius = 6371e3\n",
    "\n",
    "def haversine(row):\n",
    "    phi1         = 2 * math.pi * float(row['Latitude_x']) / 360\n",
    "    phi2         = 2 * math.pi * float(row['Latitude_y']) / 360\n",
    "    delta_phi    = 2 * math.pi * (float(row['Latitude_y']) - float(row['Latitude_x'])) / 360\n",
    "    delta_lambda = 2 * math.pi * (float(row['Longitude_y']) - float(row['Longitude_x'])) / 360\n",
    "    \n",
    "    a = (math.sin(delta_phi/2) ** 2) + \\\n",
    "        math.cos(phi1) * math.cos(phi2) * (math.sin(delta_lambda/2) ** 2)\n",
    "    \n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    \n",
    "    d = earth_radius * c\n",
    "    \n",
    "    return d / 1000\n",
    "\n",
    "prod = pd.merge(stations, stations, on='key')\n",
    "prod['dist'] = prod.apply(lambda row: haversine(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We don't consider walking to stops that are more than 1 kilometers away\n",
    "max_walking_distance = 1\n",
    "walk_df = prod[prod['dist'] <= max_walking_distance]\n",
    "walk_df = walk_df[walk_df['Remark_x'] != walk_df['Remark_y']]\n",
    "\n",
    "walk_df = walk_df[['Remark_x', 'Remark_y', 'dist']]\n",
    "walk_df['type'] = 'walk'\n",
    "walk_df['line'] = 'walk'\n",
    "walk_df['departure_day']  = 'null'\n",
    "walk_df['departure_time'] = 'null'\n",
    "walk_df['arrival_time']   = 'null'\n",
    "# We assume an average walking speed of 5 kilometers per hour\n",
    "walk_df['lateAvg'] = walk_df.apply(lambda row: 3600 * float(row['dist']) / 5, axis=1)\n",
    "walk_df['lateStd'] = 0.0\n",
    "walk_df.drop('dist', axis=1, inplace=True)\n",
    "walk_df.columns = ['src', 'dst', 'type', 'line', 'departure_day', 'departure_time', 'arrival_time', 'lateAvg', 'lateStd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "walk_edges = spark.createDataFrame(walk_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transport edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dateFormat = 'dd.MM.yyyy HH:mm'\n",
    "timeLate = (functions.unix_timestamp('AN_PROGNOSE', format=dateFormat)\n",
    "            - functions.unix_timestamp('ANKUNFTSZEIT', format=dateFormat))\n",
    "\n",
    "@functions.udf\n",
    "def clamp(late):\n",
    "    return 0 if late < 0 else late\n",
    "\n",
    "valid_stops = df.filter((df.DURCHFAHRT_TF=='false') & \n",
    "                        (df.FAELLT_AUS_TF=='false') & \n",
    "                        (df.ZUSATZFAHRT_TF=='false') &\n",
    "                        (df.AN_PROGNOSE_STATUS=='GESCHAETZT') &\n",
    "                        (df.HALTESTELLEN_NAME.isin(valid_stations))) \\\n",
    "                .select('BETRIEBSTAG',\n",
    "                        'FAHRT_BEZEICHNER', \n",
    "                        'PRODUKT_ID', \n",
    "                        'LINIEN_TEXT', \n",
    "                        'HALTESTELLEN_NAME', \n",
    "                        'AN_PROGNOSE',\n",
    "                        'ANKUNFTSZEIT', \n",
    "                        'ABFAHRTSZEIT') \\\n",
    "                .withColumn('AN_PROGNOSE',  functions.to_timestamp(df.AN_PROGNOSE, dateFormat))  \\\n",
    "                .withColumn('ANKUNFTSZEIT', functions.to_timestamp(df.ANKUNFTSZEIT, dateFormat)) \\\n",
    "                .withColumn('ABFAHRTSZEIT', functions.to_timestamp(df.ABFAHRTSZEIT, dateFormat)) \\\n",
    "                .withColumn('late', clamp(timeLate)) \\\n",
    "                .drop('AN_PROGNOSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "departures = valid_stops.filter(valid_stops.ABFAHRTSZEIT.isNotNull())\\\n",
    "                        .drop('ANKUNFTSZEIT', 'late')\n",
    "arrivals   = valid_stops.filter(valid_stops.ANKUNFTSZEIT.isNotNull())\\\n",
    "                        .drop('ABFAHRTSZEIT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arrivals.createOrReplaceTempView('arrivals')\n",
    "departures.createOrReplaceTempView('departures')\n",
    "\n",
    "joinQuery = 'SELECT d.HALTESTELLEN_NAME AS src, a.HALTESTELLEN_NAME AS dst,              \\\n",
    "                    d.PRODUKT_ID AS type, d.LINIEN_TEXT AS line,                         \\\n",
    "                    date_format(d.ABFAHRTSZEIT, \\'EEEE\\') AS departure_day,              \\\n",
    "                    SUBSTRING(d.ABFAHRTSZEIT, 12, 8) AS departure_time,                  \\\n",
    "                    SUBSTRING(a.ANKUNFTSZEIT, 12, 8) AS arrival_time,                    \\\n",
    "                    a.late                                                               \\\n",
    "             FROM arrivals AS a INNER JOIN departures AS d                               \\\n",
    "             ON a.BETRIEBSTAG == d.BETRIEBSTAG                                           \\\n",
    "             AND a.FAHRT_BEZEICHNER == d.FAHRT_BEZEICHNER                                \\\n",
    "             WHERE a.HALTESTELLEN_NAME != d.HALTESTELLEN_NAME                            \\\n",
    "             AND d.ABFAHRTSZEIT < a.ANKUNFTSZEIT'\n",
    "\n",
    "edges = spark.sql(joinQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edges.createOrReplaceTempView('edges')\n",
    "\n",
    "query = 'SELECT src, dst, type, line, departure_day, departure_time, arrival_time,              \\\n",
    "         AVG(late) AS lateAvg, STD(late) AS lateStd                                             \\\n",
    "         FROM edges GROUP BY src, dst, type, line, departure_day, departure_time, arrival_time'\n",
    "\n",
    "aggregated = spark.sql(query)\n",
    "aggregated_edges = aggregated.na.fill(0.0)\n",
    "\n",
    "all_edges = aggregated_edges.union(walk_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1457.parquet.\n: org.apache.hadoop.security.AccessControlException: Permission denied: user=kgerard, access=WRITE, inode=\"/homes/schmutz\":schmutz:hadoop:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:258)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1857)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:110)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3002)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1095)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:692)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n\tat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1603)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:953)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:950)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:960)\n\tat org.apache.spark.internal.io.FileCommitProtocol.deleteWithJob(FileCommitProtocol.scala:123)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:210)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:117)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:549)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=kgerard, access=WRITE, inode=\"/homes/schmutz\":schmutz:hadoop:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:258)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1857)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:110)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3002)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1095)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:692)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1443)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1353)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy11.delete(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:634)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy12.delete(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1601)\n\t... 37 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-69aaa95eb284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_edges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/homes/schmutz/edges'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1457.parquet.\n: org.apache.hadoop.security.AccessControlException: Permission denied: user=kgerard, access=WRITE, inode=\"/homes/schmutz\":schmutz:hadoop:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:258)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1857)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:110)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3002)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1095)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:692)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n\tat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1603)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:953)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:950)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:960)\n\tat org.apache.spark.internal.io.FileCommitProtocol.deleteWithJob(FileCommitProtocol.scala:123)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:210)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:117)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:549)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=kgerard, access=WRITE, inode=\"/homes/schmutz\":schmutz:hadoop:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:258)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1857)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:110)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3002)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1095)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:692)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1443)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1353)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy11.delete(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:634)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy12.delete(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1601)\n\t... 37 more\n"
     ]
    }
   ],
   "source": [
    "all_edges.write.parquet('/homes/schmutz/edges', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vertices.write.parquet('/homes/schmutz/vertices', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vertices = spark.read.parquet('/homes/schmutz/vertices')\n",
    "\n",
    "edges = spark.read.parquet('/homes/schmutz/edges')\n",
    "\n",
    "graph = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+---------+\n",
      "|                  id|              lon|      lat|\n",
      "+--------------------+-----------------+---------+\n",
      "|  Stallikon, Loomatt|         8.485294|47.339415|\n",
      "|  Bonstetten, Lätten|         8.455791|  47.3189|\n",
      "|Wettswil a.A., Sc...|8.473424000000001|47.327783|\n",
      "|Birmensdorf ZH, W...|         8.445963|47.356073|\n",
      "|Stallikon, Langfuren|         8.490171|47.323629|\n",
      "+--------------------+-----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+----+----+-------------+--------------+------------+-----------------+-------+\n",
      "|                 src|                 dst|type|line|departure_day|departure_time|arrival_time|          lateAvg|lateStd|\n",
      "+--------------------+--------------------+----+----+-------------+--------------+------------+-----------------+-------+\n",
      "|Zürich, Carl-Spit...|Zürich, Stodolast...|walk|walk|         null|          null|        null|243.1882193019385|    0.0|\n",
      "|Zürich, Carl-Spit...|      Zürich, Rehalp|walk|walk|         null|          null|        null|  616.09711715276|    0.0|\n",
      "|Zürich, Carl-Spit...|     Zürich, Segeten|walk|walk|         null|          null|        null|401.5413867498982|    0.0|\n",
      "|Zürich, Carl-Spit...|Zürich, Waserstrasse|walk|walk|         null|          null|        null|227.3430054944393|    0.0|\n",
      "|Zürich, Carl-Spit...| Zürich, Wiesliacher|walk|walk|         null|          null|        null|607.4434392971077|    0.0|\n",
      "+--------------------+--------------------+----+----+-------------+--------------+------------+-----------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vertices.show(n=5)\n",
    "edges.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Journey Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MINUTES_PER_DAY = 1440\n",
    "MINUTES_PER_HOUR = 60\n",
    "SECONDS_PER_MINUTE = 60\n",
    "\n",
    "def computeLengthInMinutes(departure, arrival):\n",
    "    dep = (departure[2:]).split(':')\n",
    "    arr = (arrival[2:]).split(':')\n",
    "    a = (int(arrival[:1]) - int(departure[:1])) * MINUTES_PER_DAY\n",
    "    b = (int(arr[0]) - int(dep[0])) * MINUTES_PER_HOUR\n",
    "    c = (int(arr[1]) - int(dep[1]))\n",
    "    return a + b + c\n",
    "\n",
    "def computeTime(start, duration):\n",
    "    tmp = start.split(':')\n",
    "    a = int(tmp[0][2:]) * MINUTES_PER_HOUR + int(tmp[1])\n",
    "    b = duration // SECONDS_PER_MINUTE\n",
    "    prefix = tmp[0][:2] if a + b < MINUTES_PER_DAY else '1-'\n",
    "    a = (a + b) % MINUTES_PER_DAY\n",
    "    minutes = a % MINUTES_PER_HOUR\n",
    "    hours = (a - minutes) // MINUTES_PER_HOUR\n",
    "    return prefix + \"{:02d}\".format(int(hours)) + ':' + \"{:02d}\".format(int(minutes)) + ':00'\n",
    "\n",
    "def computeProb(depTime, lateAvg, lateStd, arrTime):\n",
    "    length = computeLengthInMinutes(depTime, arrTime) * 60\n",
    "    if lateStd != 0.0:\n",
    "        return stat.norm(loc=lateAvg, scale=lateStd).cdf(length)\n",
    "    elif lateAvg <= length:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def computeDiff(departure, arrival):\n",
    "    dep = (departure[2:]).split(':')\n",
    "    arr = (arrival[2:]).split(':')\n",
    "    a = (int(arrival[:1]) - int(departure[:1])) * MINUTES_PER_DAY\n",
    "    b = (int(arr[0]) - int(dep[0])) * MINUTES_PER_HOUR\n",
    "    c = (int(arr[1]) - int(dep[1]))\n",
    "    tot = a + b + c\n",
    "    hours = tot // MINUTES_PER_HOUR\n",
    "    minutes = tot % MINUTES_PER_HOUR\n",
    "    return \"{:02d}\".format(int(hours)) + ':' + \"{:02d}\".format(int(minutes)) + ':00'\n",
    "\n",
    "def computeCost(cost, late):\n",
    "    tmp = cost.split(':')\n",
    "    a = int(tmp[0][2:]) * MINUTES_PER_HOUR + int(tmp[1])\n",
    "    b = late // SECONDS_PER_MINUTE\n",
    "    prefix = tmp[0][:2] if a > b else '0-'\n",
    "    a = (a - b) % MINUTES_PER_DAY\n",
    "    minutes = a % MINUTES_PER_HOUR\n",
    "    hours = (a - minutes) // MINUTES_PER_HOUR\n",
    "    return prefix + \"{:02d}\".format(int(hours)) + ':' + \"{:02d}\".format(int(minutes)) + ':00'\n",
    "\n",
    "def getFilteredEdges(startDay, finishDay, startTime, finishTime, duration):\n",
    "    def valid(day, depTime, arrTime, walkTime):\n",
    "        if startDay==finishDay:\n",
    "            return ((day=='null') & (walkTime<=duration)) | \\\n",
    "                    ((day==startDay) & (depTime>=startTime) & (arrTime<=finishTime) & (depTime<=arrTime))\n",
    "        else:\n",
    "            return ((day=='null') & (walkTime<=duration)) | \\\n",
    "                    (((day==startDay) & (depTime>=startTime) & ((depTime<=arrTime) | (arrTime<=finishTime))) | \\\n",
    "                     ((day==finishDay) & (depTime<finishTime) & (arrTime<=finishTime)))\n",
    "\n",
    "    return graph.filterEdges(valid(graph.edges.departure_day, \n",
    "                                graph.edges.departure_time,\n",
    "                                graph.edges.arrival_time,\n",
    "                                graph.edges.lateAvg)).edges\n",
    "\n",
    "\n",
    "def add_vertice_to_set(max_set, vertice, vertice_costs, edges, next_vertices, certain_path, earliest):\n",
    "    \n",
    "    max_set.add(vertice)\n",
    "    cost = vertice_costs[vertice]\n",
    "    \n",
    "    if earliest:\n",
    "        vertice_edges = edges.out_edges(vertice, data=True)\n",
    "\n",
    "        for parallel_paths in vertice_edges:\n",
    "            edge = parallel_paths[2]\n",
    "            if edge['type'] == 'walk':\n",
    "                new_cost = computeCost(cost, -edge['lateAvg'])\n",
    "                if (vertice not in next_vertices or next_vertices[vertice]['type'] != 'walk') and (edge['dst'] not in vertice_costs or new_cost < vertice_costs[edge['dst']]):\n",
    "                    next_vertices[edge['dst']] = edge\n",
    "                    vertice_costs[edge['dst']] = new_cost\n",
    "            elif edge['departure_time'] > cost and \\\n",
    "                (edge['dst'] not in vertice_costs or edge['arrival_time'] < vertice_costs[edge['dst']]):\n",
    "                if (not certain_path) or computeProb(edge['departure_time'],  edge['lateAvg'], edge['lateStd'], cost) == 1:\n",
    "                    vertice_costs[edge['dst']] = edge['arrival_time']\n",
    "                    next_vertices[edge['dst']] = edge\n",
    "    else:\n",
    "        vertice_edges = edges.in_edges(vertice, data=True)\n",
    "\n",
    "        for parallel_paths in vertice_edges:\n",
    "            edge = parallel_paths[2]\n",
    "            if edge['type'] == 'walk':\n",
    "                new_cost = computeCost(cost, edge['lateAvg'])\n",
    "                if (vertice not in next_vertices or next_vertices[vertice]['type'] != 'walk') and (edge['src'] not in vertice_costs or new_cost > vertice_costs[edge['src']]):\n",
    "                    next_vertices[edge['src']] = edge\n",
    "                    vertice_costs[edge['src']] = new_cost\n",
    "            elif edge['arrival_time'] < cost and \\\n",
    "                (edge['src'] not in vertice_costs or edge['departure_time'] > vertice_costs[edge['src']]):\n",
    "                if (not certain_path) or computeProb(edge['arrival_time'],  edge['lateAvg'], edge['lateStd'], cost) == 1:\n",
    "                    vertice_costs[edge['src']] = edge['departure_time']\n",
    "                    next_vertices[edge['src']] = edge\n",
    "\n",
    "def get_max_vertice_not_in_set(max_set, vertice_costs, min_trip_departure_time):\n",
    "    max_vertice = None\n",
    "    max_cost = min_trip_departure_time\n",
    "    for vertice in vertice_costs:\n",
    "        if vertice not in max_set and vertice_costs[vertice] > max_cost:\n",
    "            max_cost = vertice_costs[vertice]\n",
    "            max_vertice = vertice\n",
    "    \n",
    "    return max_vertice\n",
    "\n",
    "def get_min_vertice_not_in_set(max_set, vertice_costs, min_trip_departure_time):\n",
    "    max_vertice = None\n",
    "    max_cost = min_trip_departure_time\n",
    "    for vertice in vertice_costs:\n",
    "        if vertice not in max_set and vertice_costs[vertice] < max_cost:\n",
    "            max_cost = vertice_costs[vertice]\n",
    "            max_vertice = vertice\n",
    "    \n",
    "    return max_vertice\n",
    "\n",
    "def find_path(next_vertices, current_vertice, current_path, direction):\n",
    "    if current_vertice not in next_vertices:\n",
    "        return current_path\n",
    "    next_vertice = next_vertices[current_vertice][direction]\n",
    "    current_path.append(next_vertices[current_vertice])\n",
    "    return find_path(next_vertices, next_vertice, current_path, direction)\n",
    "    \n",
    "\n",
    "def find_shortest_path(departure_station, arrival_station, \n",
    "                       startDateTime, endDateTime, \n",
    "                       min_probability_of_sucess, get_all_destinations=False,\n",
    "                       subgraph=None, certain_path=False, earliest=False):\n",
    "    \n",
    "    startTime  = str(startDateTime.time())\n",
    "    endTime = str(endDateTime.time())\n",
    "\n",
    "    startDay  = calendar.day_name[startDateTime.weekday()]\n",
    "    endDay = calendar.day_name[endDateTime.weekday()]\n",
    "    \n",
    "    min_trip_departure_time = '0-' + startTime\n",
    "    \n",
    "    endTimePrefix = '0-' if (startDay == endDay) else '1-'\n",
    "    requested_arrival_time = endTimePrefix + endTime\n",
    "    \n",
    "    duration = (endDateTime - startDateTime).seconds\n",
    "    \n",
    "    if subgraph is None:\n",
    "        filtered_edges = getFilteredEdges(startDay, endDay, startTime, endTime, duration)\n",
    "        \n",
    "        filtered_edges = filtered_edges.toPandas()\n",
    "\n",
    "        def to_dt(time):\n",
    "            if time == 'null':\n",
    "                return 'null'\n",
    "            elif time >= startTime:\n",
    "                return '0-' + time\n",
    "            else:\n",
    "                return '1-' + time\n",
    "\n",
    "        filtered_edges['departure_time'] = filtered_edges['departure_time'].map(lambda x: to_dt(x))\n",
    "        filtered_edges['arrival_time']   = filtered_edges['arrival_time'].map(lambda x: to_dt(x))\n",
    "        \n",
    "        G = nx.from_pandas_edgelist(filtered_edges, 'src', 'dst', edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "    else:\n",
    "        G = subgraph\n",
    "        \n",
    "    # as day#-hh-mm-ss\n",
    "    vertice_costs = {}\n",
    "    max_set = set()\n",
    "    next_vertices = {}\n",
    "    if earliest:\n",
    "        vertice_costs[departure_station] = min_trip_departure_time\n",
    "        target = arrival_station\n",
    "        add_vertice_to_set(max_set, departure_station, vertice_costs, G, next_vertices, certain_path, earliest)\n",
    "        direction = 'src'\n",
    "    else:\n",
    "        vertice_costs[arrival_station] = requested_arrival_time\n",
    "        target = departure_station\n",
    "        add_vertice_to_set(max_set, arrival_station, vertice_costs, G, next_vertices, certain_path, earliest)\n",
    "        direction= 'dst'\n",
    "\n",
    "    no_solution = False\n",
    "    \n",
    "    while((target not in max_set or get_all_destinations) and not no_solution):\n",
    "        if earliest:\n",
    "            max_vertice = get_min_vertice_not_in_set(max_set, vertice_costs, requested_arrival_time)\n",
    "        else:\n",
    "            max_vertice = get_max_vertice_not_in_set(max_set, vertice_costs, min_trip_departure_time)\n",
    "        if max_vertice is None:\n",
    "            no_solution = True\n",
    "        else:\n",
    "            add_vertice_to_set(max_set, max_vertice, vertice_costs, G, next_vertices, certain_path, earliest)\n",
    "    \n",
    "    if get_all_destinations:\n",
    "        return vertice_costs\n",
    "    if no_solution:\n",
    "        return \"no solution\"\n",
    "    departure_time = vertice_costs[departure_station]\n",
    "    \n",
    "    if earliest:\n",
    "        trip_duration = computeDiff(min_trip_departure_time, vertice_costs[target])\n",
    "    else:\n",
    "        trip_duration = computeDiff(vertice_costs[target], requested_arrival_time)\n",
    "    return departure_time, trip_duration, find_path(next_vertices, target, [target], direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0-18:00:00',\n",
       " '00:51:00',\n",
       " ['Urdorf, Schlierenstrasse',\n",
       "  {'arrival_time': 'null',\n",
       "   'departure_day': 'null',\n",
       "   'departure_time': 'null',\n",
       "   'dst': 'Urdorf, Schlierenstrasse',\n",
       "   'lateAvg': 635.0863749675257,\n",
       "   'lateStd': 0.0,\n",
       "   'line': 'walk',\n",
       "   'src': 'Glanzenberg',\n",
       "   'type': 'walk'},\n",
       "  {'arrival_time': '0-18:40:00',\n",
       "   'departure_day': 'Tuesday',\n",
       "   'departure_time': '0-18:29:00',\n",
       "   'dst': 'Glanzenberg',\n",
       "   'lateAvg': 156.66666666666666,\n",
       "   'lateStd': 107.21062282749234,\n",
       "   'line': 'S3',\n",
       "   'src': 'Zürich HB',\n",
       "   'type': 'Zug'},\n",
       "  {'arrival_time': '0-18:23:00',\n",
       "   'departure_day': 'Tuesday',\n",
       "   'departure_time': '0-18:09:00',\n",
       "   'dst': 'Zürich HB',\n",
       "   'lateAvg': 6.666666666666667,\n",
       "   'lateStd': 28.284271247461902,\n",
       "   'line': 'S8',\n",
       "   'src': 'Kilchberg',\n",
       "   'type': 'Zug'}])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fromStation = 'Kilchberg'\n",
    "toStation   = 'Urdorf, Schlierenstrasse'\n",
    "startDateTime = datetime(2019, 6, 4, 18, 0)\n",
    "endDateTime   = datetime(2019, 6, 4, 19, 57)\n",
    "\n",
    "res = find_shortest_path(fromStation, toStation, \n",
    "                   startDateTime, \n",
    "                   endDateTime, 0, get_all_destinations=False, certain_path=False, earliest=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINUTES_PER_DAY = 1440\n",
    "MINUTES_PER_HOUR = 60\n",
    "SECONDS_PER_MINUTE = 60\n",
    "\n",
    "MAX_PATH_LENGTH = 4\n",
    "\n",
    "def compute_dep_time(curr_time, curr_path, edge=None):\n",
    "    if len(curr_path) == 1:\n",
    "        dep = curr_time if edge is None else edge['departure_time']\n",
    "    elif curr_path[1]['type'] == 'walk':\n",
    "        if edge is not None and len(curr_path) == 2:\n",
    "            dep = computeCost(edge['departure_time'], curr_path[1]['lateAvg'])\n",
    "        elif len(curr_path) > 2:\n",
    "            dep = computeCost(curr_path[2]['departure_time'], curr_path[1]['lateAvg'])\n",
    "        else:\n",
    "            dep = curr_time\n",
    "    else:\n",
    "        dep = curr_path[1]['departure_time']\n",
    "    \n",
    "    return dep\n",
    "\n",
    "def compute_paths_between(src, dst, edges, visited, curr_path, \n",
    "                          curr_prob, curr_time, curr_lateAvg, curr_lateStd, \n",
    "                          min_trip_departure_time, requested_arrival_time, \n",
    "                          paths, last_line_taken, max_dep_times, min_prob_success, min_duration, mode):\n",
    "    visited.add(src)\n",
    "    \n",
    "    if src == dst:\n",
    "        final_prob = computeProb(curr_time, curr_lateAvg, curr_lateStd, requested_arrival_time) * curr_prob\n",
    "        if final_prob >= min_prob_success:\n",
    "            final_path = curr_path.copy()\n",
    "            final_path.append(curr_time)\n",
    "            final_path.append(final_prob)\n",
    "            \n",
    "            dep = compute_dep_time(min_trip_departure_time, final_path[:-2], None)\n",
    "            duration = computeDiff(dep, final_path[-2])\n",
    "            if mode == 'both':\n",
    "                if duration < min_duration['min']:\n",
    "                    min_duration['min'] = duration\n",
    "            elif mode == 'arrival':\n",
    "                if dep > min_duration['dep']:\n",
    "                    min_duration['dep'] = dep\n",
    "            \n",
    "            paths.append(final_path)\n",
    "            \n",
    "    elif len(curr_path) < MAX_PATH_LENGTH:\n",
    "        vertice_edges = edges.out_edges(src, data=True)\n",
    "        for vertice_edge in vertice_edges:\n",
    "            edge = vertice_edge[2]\n",
    "            if edge['dst'] not in visited and edge['line'] != last_line_taken:\n",
    "                \n",
    "                if edge['type'] == 'walk':\n",
    "                    new_time = computeTime(curr_time, edge['lateAvg'])\n",
    "                    \n",
    "                    dep = compute_dep_time(curr_time, curr_path)\n",
    "                    duration = computeDiff(dep, new_time)\n",
    "                    \n",
    "                    if (mode == 'both' and duration <= min_duration['min']) or \\\n",
    "                       (mode == 'arrival'):\n",
    "                        if new_time <= requested_arrival_time and \\\n",
    "                           edge['dst'] in max_dep_times and new_time <= max_dep_times[edge['dst']]:\n",
    "\n",
    "                            curr_path.append(edge)\n",
    "                            compute_paths_between(edge['dst'], dst, edges, visited, curr_path, \n",
    "                                                  curr_prob, new_time, curr_lateAvg, curr_lateStd, \n",
    "                                                  min_trip_departure_time, requested_arrival_time, paths, \n",
    "                                                  edge['line'], max_dep_times, min_prob_success, min_duration, mode)\n",
    "                            curr_path.pop();\n",
    "                        \n",
    "                elif edge['departure_time'] >= curr_time and edge['dst'] in max_dep_times and \\\n",
    "                     edge['arrival_time'] <= max_dep_times[edge['dst']]:\n",
    "                        \n",
    "                    dep = compute_dep_time(curr_time, curr_path, edge = edge)\n",
    "                    duration = computeDiff(dep, edge['arrival_time'])\n",
    "                    \n",
    "                    prob = computeProb(curr_time, curr_lateAvg, curr_lateStd, edge['departure_time'])\n",
    "                    new_prob = curr_prob * prob\n",
    "                    \n",
    "                    if (mode == 'both' and duration <= min_duration['min']) or \\\n",
    "                       (mode == 'arrival' and dep >= min_duration['dep']):\n",
    "                        if new_prob >= min_prob_success:\n",
    "                            curr_path.append(edge)\n",
    "                            compute_paths_between(edge['dst'], dst, edges, visited, curr_path, \n",
    "                                                  new_prob, edge['arrival_time'], edge['lateAvg'], edge['lateStd'],\n",
    "                                                  min_trip_departure_time, requested_arrival_time, paths, \n",
    "                                                  edge['line'], max_dep_times, min_prob_success, min_duration, mode)\n",
    "                            curr_path.pop();\n",
    "        \n",
    "    visited.remove(src)\n",
    "    \n",
    "    \n",
    "def dfs(departure_station, arrival_station, \n",
    "        startDateTime, endDateTime, \n",
    "        min_probability_of_sucess, mode='both'):\n",
    "    \n",
    "    startTime  = str(startDateTime.time())\n",
    "    endTime = str(endDateTime.time())\n",
    "\n",
    "    startDay  = calendar.day_name[startDateTime.weekday()]\n",
    "    endDay = calendar.day_name[endDateTime.weekday()]\n",
    "    \n",
    "    min_trip_departure_time = '0-' + startTime\n",
    "    \n",
    "    endTimePrefix = '0-' if (startDay == endDay) else '1-'\n",
    "    requested_arrival_time = endTimePrefix + endTime\n",
    "    \n",
    "    duration = (endDateTime - startDateTime).seconds\n",
    "    \n",
    "    filtered_edges = getFilteredEdges(startDay, endDay, startTime, endTime, duration).toPandas()\n",
    "    \n",
    "    def to_dt(time):\n",
    "        if time == 'null':\n",
    "            return 'null'\n",
    "        elif time >= startTime:\n",
    "            return '0-' + time\n",
    "        else:\n",
    "            return '1-' + time\n",
    "\n",
    "    \n",
    "    filtered_edges['departure_time'] = filtered_edges['departure_time'].map(lambda x: to_dt(x))\n",
    "    filtered_edges['arrival_time']   = filtered_edges['arrival_time'].map(lambda x: to_dt(x))\n",
    "    \n",
    "    G = nx.from_pandas_dataframe(filtered_edges, 'src', 'dst', edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "    \n",
    "    max_dep_times = find_shortest_path(departure_station, arrival_station, \n",
    "                                       startDateTime, \n",
    "                                       endDateTime, min_probability_of_sucess, \n",
    "                                       get_all_destinations=True, \n",
    "                                       subgraph=G)\n",
    "    \n",
    "    fastest_certain_path = find_shortest_path(fromStation, toStation, \n",
    "                                              startDateTime, endDateTime, 0, \n",
    "                                              get_all_destinations=False, certain_path=True,\n",
    "                                              subgraph=G)\n",
    "    \n",
    "    visited = set()\n",
    "    curr_time = min_trip_departure_time\n",
    "    curr_path = [departure_station]\n",
    "    paths = []\n",
    "    \n",
    "    if fastest_certain_path != \"no solution\":\n",
    "        if mode == 'both':\n",
    "            min_duration = {'min': fastest_certain_path[1]}\n",
    "        elif mode == 'arrival':\n",
    "            min_duration = {'dep': fastest_certain_path[0]}\n",
    "    else:\n",
    "        if mode == 'both':\n",
    "            min_duration = {'min': '24:00:00'}\n",
    "        elif mode == 'arrival':\n",
    "            min_duration = {'dep': min_trip_departure_time}\n",
    "    \n",
    "    compute_paths_between(departure_station, arrival_station, G, \n",
    "                          visited, curr_path, 1.0, curr_time, 0.0, 0.0, min_trip_departure_time, \n",
    "                          requested_arrival_time, paths, '', max_dep_times, \n",
    "                          min_probability_of_sucess, min_duration, mode)\n",
    "    \n",
    "    if len(paths) == 0:\n",
    "        return {'departure time' : '', 'arrival_time' : '', \n",
    "                'duration' : '', 'path': []}\n",
    "    \n",
    "    times = [computeDiff(compute_dep_time(min_trip_departure_time, path[:-2], None), path[-2]) for path in paths]\n",
    "    dep_times = [compute_dep_time(min_trip_departure_time, path[:-2], None) for path in paths]\n",
    "    \n",
    "    if mode == 'both':\n",
    "        best_path_idx = np.argmin(times)\n",
    "    elif mode == 'arrival':\n",
    "        best_path_idx = np.argmax(dep_times)\n",
    "    \n",
    "    best_path = paths[best_path_idx]\n",
    "    \n",
    "    path_edges = best_path[1:-2]\n",
    "    \n",
    "    # Compute departure and arrival time for walk edges and removing unnecessary data\n",
    "    for idx, edge in enumerate(path_edges):\n",
    "        if edge['type'] == 'walk':\n",
    "            if idx == 0:\n",
    "                if len(path_edges) == 1:\n",
    "                    edge['arrival_time'] = requested_arrival_time\n",
    "                else:\n",
    "                    edge['arrival_time'] = path_edges[idx + 1]['departure_time']\n",
    "                    \n",
    "                edge['departure_time'] = computeCost(edge['arrival_time'], edge['lateAvg'])\n",
    "            else:\n",
    "                edge['departure_time'] = path_edges[idx - 1]['arrival_time']\n",
    "                edge['arrival_time'] = computeTime(edge['departure_time'], edge['lateAvg'])\n",
    "            \n",
    "            edge.pop('line');\n",
    "                \n",
    "        edge.pop('departure_day');\n",
    "        edge.pop('lateAvg');\n",
    "        edge.pop('lateStd');\n",
    "    \n",
    "    # Remove prefix from edges\n",
    "    for edge in path_edges:\n",
    "        edge['departure_time'] = edge['departure_time'][2:]\n",
    "        edge['arrival_time'] = edge['arrival_time'][2:]\n",
    "    \n",
    "    departure_time = path_edges[0]['departure_time']\n",
    "    arrival_time = path_edges[-1]['arrival_time']\n",
    "    \n",
    "    return {'departure time' : departure_time, 'arrival_time' : arrival_time, \n",
    "            'duration' : times[best_path_idx], 'path': path_edges}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arrival_time': '19:05:00',\n",
       " 'departure time': '18:26:00',\n",
       " 'duration': '00:39:00',\n",
       " 'path': [{'arrival_time': '18:39:00',\n",
       "   'departure_time': '18:26:00',\n",
       "   'dst': 'Zürich HB',\n",
       "   'line': 'S24',\n",
       "   'src': 'Kilchberg',\n",
       "   'type': 'Zug'},\n",
       "  {'arrival_time': '18:55:00',\n",
       "   'departure_time': '18:44:00',\n",
       "   'dst': 'Glanzenberg',\n",
       "   'line': 'S12',\n",
       "   'src': 'Zürich HB',\n",
       "   'type': 'Zug'},\n",
       "  {'arrival_time': '19:05:00',\n",
       "   'departure_time': '18:55:00',\n",
       "   'dst': 'Urdorf, Schlierenstrasse',\n",
       "   'src': 'Glanzenberg',\n",
       "   'type': 'walk'}]}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fromStation = 'Kilchberg'\n",
    "toStation   = 'Urdorf, Schlierenstrasse'\n",
    "startDateTime = datetime(2019, 6, 4, 18, 20)\n",
    "endDateTime   = datetime(2019, 6, 4, 19, 57)\n",
    "\n",
    "res = dfs(fromStation, toStation, \n",
    "          startDateTime, \n",
    "          endDateTime, 0.95, mode='both')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARCHIVED CODE FOR POSSIBLE REUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSubGraph(graph, startDay, finishDay, startTime, finishTime, duration):\n",
    "    def valid(day, depTime, arrTime, walkTime):\n",
    "        if startDay==finishDay:\n",
    "            return ((day=='null') & (walkTime<=duration)) | \\\n",
    "                    ((day==startDay) & (depTime>=startTime) & (arrTime<=finishTime) & (depTime<=arrTime))\n",
    "        else:\n",
    "            return ((day=='null') & (walkTime<=duration)) | \\\n",
    "                    (((day==startDay) & (depTime>=startTime) & ((depTime<=arrTime) | (arrTime<=finishTime))) | \\\n",
    "                     ((day==finishDay) & (depTime<finishTime) & (arrTime<=finishTime)))\n",
    "\n",
    "    return graph.filterEdges(valid(graph.edges.departure_day, \n",
    "                                graph.edges.departure_time,\n",
    "                                graph.edges.arrival_time,\n",
    "                                graph.edges.lateAvg))  \\\n",
    "                .dropIsolatedVertices()\n",
    "\n",
    "def howFarNaive(graph, fromStation, startDateTime, duration):\n",
    "    \n",
    "    if duration >= 120:\n",
    "        print('You can walk anywhere in that time')\n",
    "        return\n",
    "    \n",
    "    finishDateTime = startDateTime + timedelta(minutes=duration)\n",
    "\n",
    "    startTime  = str(startDateTime.time())\n",
    "    finishTime = str(finishDateTime.time())\n",
    "\n",
    "    startDay  = calendar.day_name[startDateTime.weekday()]\n",
    "    finishDay = calendar.day_name[finishDateTime.weekday()]\n",
    "\n",
    "    print(startDay, startTime)\n",
    "    print(finishDay, finishTime)\n",
    "    \n",
    "    @functions.udf\n",
    "    def addTime(arr_time, dep_time, late):\n",
    "        if arr_time=='null':\n",
    "            tmp = dep_time.split(':')\n",
    "            return str((datetime.combine(date.today(), dt.time(int(tmp[0]), int(tmp[1]), int(tmp[2]))) + \n",
    "                        timedelta(seconds=int(late))).time())\n",
    "        else:\n",
    "            return arr_time\n",
    "    \n",
    "    @functions.udf\n",
    "    def checkDay(day, dep_time, arr_time):\n",
    "        return finishDay if arr_time<dep_time else day\n",
    "    \n",
    "    @functions.udf\n",
    "    def checkWalk(ttype):\n",
    "        return 1 if ttype=='walk' else 0\n",
    "    \n",
    "    @functions.udf\n",
    "    def checkIfValid(arr_time, day):\n",
    "        tmp = arr_time.split(':')\n",
    "        arr_date = startDateTime.date() if day==calendar.day_name[startDateTime.weekday()] else finishDateTime.date()\n",
    "        arrival = datetime.combine(arr_date, dt.time(int(tmp[0]), int(tmp[1]), int(tmp[2])))\n",
    "        return arrival < finishDateTime\n",
    "    \n",
    "    reachable = vertices.filter(vertices.id==fromStation)             \\\n",
    "                        .withColumn('time', functions.lit(startTime)) \\\n",
    "                        .withColumn('day', functions.lit(startDay))   \\\n",
    "                        .withColumn('just_walked', functions.lit(0))\n",
    "    \n",
    "    g = getSubGraph(graph, startDay, finishDay, startTime, finishTime, 60*duration)\n",
    "    g.persist();\n",
    "    g.edges.createOrReplaceTempView('edges')\n",
    "    g.vertices.createOrReplaceTempView('vertices')\n",
    "    \n",
    "    curr = reachable\n",
    "    \n",
    "    #while len(curr.head(1)) > 0:\n",
    "    for i in range(1):\n",
    "        curr.createOrReplaceTempView('curr')\n",
    "\n",
    "        query = 'SELECT v.*, c.time AS past_time, c.just_walked, c.day, e.type,          \\\n",
    "                        e.departure_time, e.arrival_time, e.lateAvg                      \\\n",
    "                 FROM curr AS c INNER JOIN edges AS e INNER JOIN vertices AS v           \\\n",
    "                 ON c.id==e.src                                                          \\\n",
    "                 AND e.dst==v.id                                                         \\\n",
    "                 WHERE (e.type!=\\'walk\\' OR c.just_walked==0)                            \\\n",
    "                 AND (e.type==\\'walk\\'                                                   \\\n",
    "                 OR (e.departure_time>=c.time AND c.day==e.departure_day)                \\\n",
    "                 OR (e.departure_time<c.time AND c.day!=e.departure_day))'\n",
    "\n",
    "        curr = spark.sql(query).withColumn('time', addTime('arrival_time', 'past_time', 'lateAvg')) \\\n",
    "                               .withColumn('day', checkDay('day', 'past_time', 'time'))             \\\n",
    "                               .filter(checkIfValid('time', 'day')=='true')                         \\\n",
    "                               .withColumn('just_walked', checkWalk('type'))                        \\\n",
    "                               .select('id', 'lon', 'lat', 'time', 'day', 'just_walked')\n",
    "        curr.persist()\n",
    "        reachable = reachable.union(curr)\n",
    "    \n",
    "    @functions.udf\n",
    "    def computeRadius(arr_time, day):\n",
    "        tmp = arr_time.split(':')\n",
    "        arr_date = startDateTime.date() if day==calendar.day_name[startDateTime.weekday()] else finishDateTime.date()\n",
    "        arrival = datetime.combine(arr_date, dt.time(int(tmp[0]), int(tmp[1]), int(tmp[2])))\n",
    "        return (finishDateTime - arrival).seconds * 5 // 3.6\n",
    "    \n",
    "    reachable = reachable.withColumn('radius', computeRadius('time', 'day')) \\\n",
    "                         .select('id', 'lon', 'lat', 'radius', 'time', 'just_walked')                     \\\n",
    "                         .toPandas()\n",
    "    \n",
    "    g.unpersist();\n",
    "    \n",
    "    return reachable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friday 23:45:00\n",
      "Friday 23:56:00\n",
      "9.501955032348633\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>radius</th>\n",
       "      <th>time</th>\n",
       "      <th>just_walked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dietlikon</td>\n",
       "      <td>8.619255</td>\n",
       "      <td>47.420195</td>\n",
       "      <td>916.0</td>\n",
       "      <td>23:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dietlikon, Bahnhof</td>\n",
       "      <td>8.619087</td>\n",
       "      <td>47.420359</td>\n",
       "      <td>895.0</td>\n",
       "      <td>23:45:15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brüttisellen, Gsellhof</td>\n",
       "      <td>8.629667</td>\n",
       "      <td>47.421708</td>\n",
       "      <td>116.0</td>\n",
       "      <td>23:54:36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dietlikon, Bahnhof/Bad</td>\n",
       "      <td>8.620805</td>\n",
       "      <td>47.421960</td>\n",
       "      <td>688.0</td>\n",
       "      <td>23:47:44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dietlikon, Brandbachstrasse</td>\n",
       "      <td>8.624657</td>\n",
       "      <td>47.416965</td>\n",
       "      <td>374.0</td>\n",
       "      <td>23:51:30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dietlikon, Dornenstrasse</td>\n",
       "      <td>8.616896</td>\n",
       "      <td>47.416585</td>\n",
       "      <td>477.0</td>\n",
       "      <td>23:50:16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dietlikon, Dübendorferstrasse</td>\n",
       "      <td>8.619558</td>\n",
       "      <td>47.413509</td>\n",
       "      <td>173.0</td>\n",
       "      <td>23:53:55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dietlikon, Hofwiesen</td>\n",
       "      <td>8.618518</td>\n",
       "      <td>47.426391</td>\n",
       "      <td>226.0</td>\n",
       "      <td>23:53:17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dietlikon, Industriestrasse</td>\n",
       "      <td>8.621946</td>\n",
       "      <td>47.414249</td>\n",
       "      <td>226.0</td>\n",
       "      <td>23:53:17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dietlikon, Fuchshalde</td>\n",
       "      <td>8.612612</td>\n",
       "      <td>47.420029</td>\n",
       "      <td>416.0</td>\n",
       "      <td>23:51:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Dietlikon, In Lampitzäckern</td>\n",
       "      <td>8.614949</td>\n",
       "      <td>47.417226</td>\n",
       "      <td>454.0</td>\n",
       "      <td>23:50:33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dietlikon, Lettenstrasse</td>\n",
       "      <td>8.611864</td>\n",
       "      <td>47.424570</td>\n",
       "      <td>179.0</td>\n",
       "      <td>23:53:51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dietlikon, Riedenerstrasse</td>\n",
       "      <td>8.614148</td>\n",
       "      <td>47.422982</td>\n",
       "      <td>423.0</td>\n",
       "      <td>23:50:55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dietlikon, Zentrum</td>\n",
       "      <td>8.618244</td>\n",
       "      <td>47.423012</td>\n",
       "      <td>594.0</td>\n",
       "      <td>23:48:52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Wallisellen</td>\n",
       "      <td>8.591911</td>\n",
       "      <td>47.412717</td>\n",
       "      <td>83.0</td>\n",
       "      <td>23:55:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id       lon        lat radius      time  \\\n",
       "0                       Dietlikon  8.619255  47.420195  916.0  23:45:00   \n",
       "1              Dietlikon, Bahnhof  8.619087  47.420359  895.0  23:45:15   \n",
       "2          Brüttisellen, Gsellhof  8.629667  47.421708  116.0  23:54:36   \n",
       "3          Dietlikon, Bahnhof/Bad  8.620805  47.421960  688.0  23:47:44   \n",
       "4     Dietlikon, Brandbachstrasse  8.624657  47.416965  374.0  23:51:30   \n",
       "5        Dietlikon, Dornenstrasse  8.616896  47.416585  477.0  23:50:16   \n",
       "6   Dietlikon, Dübendorferstrasse  8.619558  47.413509  173.0  23:53:55   \n",
       "7            Dietlikon, Hofwiesen  8.618518  47.426391  226.0  23:53:17   \n",
       "8     Dietlikon, Industriestrasse  8.621946  47.414249  226.0  23:53:17   \n",
       "9           Dietlikon, Fuchshalde  8.612612  47.420029  416.0  23:51:00   \n",
       "10    Dietlikon, In Lampitzäckern  8.614949  47.417226  454.0  23:50:33   \n",
       "11       Dietlikon, Lettenstrasse  8.611864  47.424570  179.0  23:53:51   \n",
       "12     Dietlikon, Riedenerstrasse  8.614148  47.422982  423.0  23:50:55   \n",
       "13             Dietlikon, Zentrum  8.618244  47.423012  594.0  23:48:52   \n",
       "14                    Wallisellen  8.591911  47.412717   83.0  23:55:00   \n",
       "\n",
       "   just_walked  \n",
       "0            0  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "5            1  \n",
       "6            1  \n",
       "7            1  \n",
       "8            1  \n",
       "9            1  \n",
       "10           1  \n",
       "11           1  \n",
       "12           1  \n",
       "13           1  \n",
       "14           0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = GraphFrame(vertices, edges)\n",
    "fromStation = 'Dietlikon'\n",
    "startDateTime  = datetime(2019, 5, 31, 23, 45)\n",
    "duration = 11\n",
    "\n",
    "start = time.time()\n",
    "reachable = howFarNaive(graph, fromStation, startDateTime, duration)\n",
    "print(time.time() - start)\n",
    "reachable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
